{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Large Language Model - Model\n",
    "\n",
    "In this workshop, you will learn how to fine tune the prompts and the LLMs to enhance and improves its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, GenerationConfig, TrainingArguments\n",
    "\n",
    "from peft import PeftModel, LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load and explore the following datasets\n",
    "\n",
    "dataset_name = \"knkarthick/dialogsum\"\n",
    "model_name = \"google/flan-t5-small\"\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': (12460, 4), 'validation': (500, 4), 'test': (1500, 4)}\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
      "        num_rows: 12460\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print a record\n",
    "print(dataset.shape)\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning the LLM model\n",
    "\n",
    "In this workshop we will be turning the <code>google/flan-t5-base</code> model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to dump a model's tunable parameters\n",
    "\n",
    "def print_trainable_model_params(model):\n",
    "   trainable_model_params = 0\n",
    "   all_model_params = 0\n",
    "   for _, param in model.named_parameters():\n",
    "      all_model_params += param.numel()\n",
    "      if param.requires_grad:\n",
    "         trainable_model_params += param.numel()\n",
    "   return f\"Trainable parameters: {trainable_model_params}\\nTotal parameters: {all_model_params}\\nPercentable of trainable parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load model\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trainable parameters: 247577856\\nTotal parameters: 247577856\\nPercentable of trainable parameters: 100.00%'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Print number of trainable parameters\n",
    "print_trainable_model_params(original_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person1#: I have a problem with my cable.\n",
      "#Person2#: What about it?\n",
      "#Person1#: My cable has been out for the past week or so.\n",
      "#Person2#: The cable is down right now. I am very sorry.\n",
      "#Person1#: When will it be working again?\n",
      "#Person2#: It should be back on in the next couple of days.\n",
      "#Person1#: Do I still have to pay for the cable?\n",
      "#Person2#: We're going to give you a credit while the cable is down.\n",
      "#Person1#: So, I don't have to pay for it?\n",
      "#Person2#: No, not until your cable comes back on.\n",
      "#Person1#: Okay, thanks for everything.\n",
      "#Person2#: You're welcome, and I apologize for the inconvenience.\n",
      "----------\n",
      "#Person1# has a problem with the cable. #Person2# promises it should work again and #Person1# doesn't have to pay while it's down.\n"
     ]
    }
   ],
   "source": [
    "idx = 100\n",
    "print(dataset['train'][idx]['dialogue'])\n",
    "print('----------')\n",
    "print(dataset['train'][idx]['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the dialogue dataset\n",
    "\n",
    "We will train the model to summarize dialogue by creating a dialogue-summary pair for the LLM to process. The dialogue is the training data and the summary is the label. This is supervized learning.\n",
    "\n",
    "The prompt will be as follows\n",
    "\n",
    "```\n",
    "Summarize the following dialogue.\\n\n",
    "\\n\n",
    "Fred: ...\\n\n",
    "Barney: ...\\n\n",
    "\\n\n",
    "Summary:\\n\n",
    "Summary of the conversation between Fred and Barney\n",
    "```\n",
    "\n",
    "The prompt and the summary will be tokenized for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utitlity function to prepare the data for training \n",
    "# Tokenize function\n",
    "def tokenize_fn(data):\n",
    "   start_prompt = 'Summarize the following dialogue.\\n\\n'\n",
    "   end_prompt = '\\n\\nSummary:'\n",
    "   prompt = [ start_prompt + d + end_prompt for d in data['dialogue'] ]\n",
    "   summary = data['summary']\n",
    "   data['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "   data['labels'] = tokenizer(summary, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "   return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: prepare the data for training with the tokenize_fn function\n",
    "tokenized_dataset = dataset.map(\n",
    "   tokenize_fn, batched=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value = id\n",
      "train_100\n",
      "---------------\n",
      "value = dialogue\n",
      "#Person1#: I have a problem with my cable.\n",
      "#Person2#: What about it?\n",
      "#Person1#: My cable has been out for the past week or so.\n",
      "#Person2#: The cable is down right now. I am very sorry.\n",
      "#Person1#: When will it be working again?\n",
      "#Person2#: It should be back on in the next couple of days.\n",
      "#Person1#: Do I still have to pay for the cable?\n",
      "#Person2#: We're going to give you a credit while the cable is down.\n",
      "#Person1#: So, I don't have to pay for it?\n",
      "#Person2#: No, not until your cable comes back on.\n",
      "#Person1#: Okay, thanks for everything.\n",
      "#Person2#: You're welcome, and I apologize for the inconvenience.\n",
      "---------------\n",
      "value = summary\n",
      "#Person1# has a problem with the cable. #Person2# promises it should work again and #Person1# doesn't have to pay while it's down.\n",
      "---------------\n",
      "value = topic\n",
      "cable\n",
      "---------------\n",
      "value = input_ids\n",
      "[12198, 1635, 1737, 8, 826, 7478, 5, 1713, 345, 13515, 536, 4663, 10, 27, 43, 3, 9, 682, 28, 82, 4807, 5, 1713, 345, 13515, 357, 4663, 10, 363, 81, 34, 58, 1713, 345, 13515, 536, 4663, 10, 499, 4807, 65, 118, 91, 21, 8, 657, 471, 42, 78, 5, 1713, 345, 13515, 357, 4663, 10, 37, 4807, 19, 323, 269, 230, 5, 27, 183, 182, 8032, 5, 1713, 345, 13515, 536, 4663, 10, 366, 56, 34, 36, 464, 541, 58, 1713, 345, 13515, 357, 4663, 10, 94, 225, 36, 223, 30, 16, 8, 416, 1158, 13, 477, 5, 1713, 345, 13515, 536, 4663, 10, 531, 27, 341, 43, 12, 726, 21, 8, 4807, 58, 1713, 345, 13515, 357, 4663, 10, 101, 31, 60, 352, 12, 428, 25, 3, 9, 998, 298, 8, 4807, 19, 323, 5, 1713, 345, 13515, 536, 4663, 10, 264, 6, 27, 278, 31, 17, 43, 12, 726, 21, 34, 58, 1713, 345, 13515, 357, 4663, 10, 465, 6, 59, 552, 39, 4807, 639, 223, 30, 5, 1713, 345, 13515, 536, 4663, 10, 16036, 6, 2049, 21, 762, 5, 1713, 345, 13515, 357, 4663, 10, 148, 31, 60, 2222, 6, 11, 27, 22103, 21, 8, 25741, 5, 20698, 10, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "---------------\n",
      "value = labels\n",
      "[1713, 345, 13515, 536, 4663, 65, 3, 9, 682, 28, 8, 4807, 5, 1713, 345, 13515, 357, 4663, 11236, 34, 225, 161, 541, 11, 1713, 345, 13515, 536, 4663, 744, 31, 17, 43, 12, 726, 298, 34, 31, 7, 323, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "for k, v in enumerate(tokenized_dataset['train'][idx]):\n",
    "   #print(f'key = {k}')\n",
    "   print(f'value = {v}')\n",
    "   print(f'{tokenized_dataset[\"train\"][idx][v]}')\n",
    "   print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person1# has a problem with the cable. #Person2# promises it should work again and #Person1# doesn't have to pay while it's down.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.decode(tokenized_dataset['train'][idx]['labels'])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Verify prepared data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove id, dialogue, summary and topic columns from dataset. We only want input_ids and labels\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(\n",
    "   ['id', 'dialogue', 'summary', 'topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value = input_ids\n",
      "[12198, 1635, 1737, 8, 826, 7478, 5, 1713, 345, 13515, 536, 4663, 10, 27, 43, 3, 9, 682, 28, 82, 4807, 5, 1713, 345, 13515, 357, 4663, 10, 363, 81, 34, 58, 1713, 345, 13515, 536, 4663, 10, 499, 4807, 65, 118, 91, 21, 8, 657, 471, 42, 78, 5, 1713, 345, 13515, 357, 4663, 10, 37, 4807, 19, 323, 269, 230, 5, 27, 183, 182, 8032, 5, 1713, 345, 13515, 536, 4663, 10, 366, 56, 34, 36, 464, 541, 58, 1713, 345, 13515, 357, 4663, 10, 94, 225, 36, 223, 30, 16, 8, 416, 1158, 13, 477, 5, 1713, 345, 13515, 536, 4663, 10, 531, 27, 341, 43, 12, 726, 21, 8, 4807, 58, 1713, 345, 13515, 357, 4663, 10, 101, 31, 60, 352, 12, 428, 25, 3, 9, 998, 298, 8, 4807, 19, 323, 5, 1713, 345, 13515, 536, 4663, 10, 264, 6, 27, 278, 31, 17, 43, 12, 726, 21, 34, 58, 1713, 345, 13515, 357, 4663, 10, 465, 6, 59, 552, 39, 4807, 639, 223, 30, 5, 1713, 345, 13515, 536, 4663, 10, 16036, 6, 2049, 21, 762, 5, 1713, 345, 13515, 357, 4663, 10, 148, 31, 60, 2222, 6, 11, 27, 22103, 21, 8, 25741, 5, 20698, 10, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "---------------\n",
      "value = labels\n",
      "[1713, 345, 13515, 536, 4663, 65, 3, 9, 682, 28, 8, 4807, 5, 1713, 345, 13515, 357, 4663, 11236, 34, 225, 161, 541, 11, 1713, 345, 13515, 536, 4663, 744, 31, 17, 43, 12, 726, 298, 34, 31, 7, 323, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# TODO: Verify dataset again\n",
    "for k, v in enumerate(tokenized_dataset['train'][idx]):\n",
    "   #print(f'key = {k}')\n",
    "   print(f'value = {v}')\n",
    "   print(f'{tokenized_dataset[\"train\"][idx][v]}')\n",
    "   print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune model with pre-processed dataset\n",
    "\n",
    "We will use [<code>Trainer</code>](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#api-reference%20][%20transformers.Trainer) to train the original model. The training result will be written out. The trainer will be configure with [<code>TrainingArgument</code>](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available:  False\n"
     ]
    }
   ],
   "source": [
    "# CUDA information\n",
    "\n",
    "print('CUDA available: ', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "   print('B16 supported: ', torch.cuda.is_bf16_supported())\n",
    "   torch.cuda.set_device(0)\n",
    "   print('Current device: ', torch.cuda.current_device())\n",
    "   print('CUDA device name: ', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning the LLM Model with Low-Rank Adaptation (LoRA) / Parameter Efficient Fine Tuning (PEFT)\n",
    "\n",
    "We will add a LoRA adapter to the LLM (flan-t5-base) and train the adapter. The original LLM will be frozen. The adapter can be combined with the original LLM during inferencing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 768)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-11): 11 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-11): 11 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(original_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "   r=16,\n",
    "   lora_alpha=16,\n",
    "   target_modules=['q', 'v'],\n",
    "   task_type=TaskType.SEQ_2_SEQ_LM   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add LoRA to the LLM model to be trained\n",
    "lora_model = get_peft_model(original_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 1769472\n",
      "Total parameters: 249347328\n",
      "Percentable of trainable parameters: 0.71%\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print number of parameters, compare LoRA to the original model\n",
    "print(print_trainable_model_params(lora_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train model with LoRA\n",
    "output_dir = f'peft-dialog-{str(int(time.time()))}'\n",
    "lora_training_args = TrainingArguments(\n",
    "   output_dir=output_dir,\n",
    "   auto_find_batch_size=True,\n",
    "   learning_rate=1e-3, # 0.0001\n",
    "   num_train_epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create trainer and train model\n",
    "lora_trainer = Trainer(\n",
    "   model = lora_model,\n",
    "   args = lora_training_args,\n",
    "   train_dataset = tokenized_dataset['train'],\n",
    "   eval_dataset = tokenized_dataset['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: train - cannot run\n",
    "lora_trainer.train()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "lora_trainer.save_pretrained(\"mymodel\")\n",
    "tokenizer.save_pretrained(\"mymodel\")\n",
    "\n",
    "# Upload to Hugging Face\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a trained LoRA model\n",
    "\n",
    "The training will take a few hours and over many iterations.\n",
    "\n",
    "For the purpose of this workshop we use a save model [intotheverse/peft-dialogue-summary-checkpoint](https://huggingface.co/intotheverse/peft-dialogue-summary-checkpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Load the original model and add the pre-trained LoRA adaptation to the model\n",
    "peft_dialogue_summary_checkpoint = 'intotheverse/peft-dialogue-summary-checkpoint'\n",
    "\n",
    "# Load base model and tokenizer\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenzer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the LoRA over base_model\n",
    "lora_model = PeftModel.from_pretrained(base_model, peft_dialogue_summary_checkpoint,\n",
    "      torch_dtype=torch.bfloat16)\n",
    "\n",
    "# Load the original model\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate LoRA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate LoRA model against the original \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n",
      "i = 1\n",
      "i = 2\n",
      "i = 3\n",
      "i = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>original_model_summary</th>\n",
       "      <th>lora_model_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>The memo is to be distributed to all employees...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>The memo is to be distributed to all employees...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>The memo is to be distributed to all employees...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "      <td>#Person2# got stuck in traffic and #Person1# s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "      <td>#Person2# got stuck in traffic and #Person1# s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               label  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  In order to prevent employees from wasting tim...   \n",
       "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3  #Person2# arrives late because of traffic jam....   \n",
       "4  #Person2# decides to follow #Person1#'s sugges...   \n",
       "\n",
       "                              original_model_summary  \\\n",
       "0  The memo is to be distributed to all employees...   \n",
       "1  The memo is to be distributed to all employees...   \n",
       "2  The memo is to be distributed to all employees...   \n",
       "3  The traffic jam at the Carrefour intersection ...   \n",
       "4  The traffic jam at the Carrefour intersection ...   \n",
       "\n",
       "                                  lora_model_summary  \n",
       "0  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "1  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "2  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "3  #Person2# got stuck in traffic and #Person1# s...  \n",
       "4  #Person2# got stuck in traffic and #Person1# s...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data for evaluation\n",
    "dialogues = []\n",
    "summaries = []\n",
    "orig_model_summaries = []\n",
    "lora_model_summaries = []\n",
    "config = GenerationConfig(max_new_tokens=200)\n",
    "\n",
    "for i in range(5):\n",
    "   print(f'i = {i}')\n",
    "   d = dataset['test'][i]['dialogue']\n",
    "   s = dataset['test'][i]['summary']\n",
    "   prompt = f\"Summarize the following conversation.\\n\\n{d}\\n\\nSummary:\"\n",
    "   tokenized_prompt = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "   orig_resp = original_model.generate(input_ids=tokenized_prompt, generation_config=config)\n",
    "   orig_resp_text = tokenizer.decode(orig_resp[0], skip_special_tokens=True)\n",
    "   lora_resp = lora_model.generate(input_ids=tokenized_prompt, generation_config=config)\n",
    "   lora_resp_text = tokenizer.decode(lora_resp[0], skip_special_tokens=True)\n",
    "\n",
    "   summaries.append(s)\n",
    "   orig_model_summaries.append(orig_resp_text)\n",
    "   lora_model_summaries.append(lora_resp_text)\n",
    "\n",
    "zipped_summaries = list(zip(summaries, orig_model_summaries, lora_model_summaries))\n",
    "df = pd.DataFrame(zipped_summaries, columns=['label', 'original_model_summary', 'lora_model_summary'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models with ROUGE/Bleu metrics\n",
    "\n",
    "Recall-Oriented Understudy for Gisting Evaluate ([ROUGE](https://pub.aimind.so/unveiling-the-power-of-rouge-metrics-in-nlp-b6d3f96d3363)) is a set of metrics used to evaluate the quality of machine-generated text, such as summaries and translations. ROUGE metrics compare the generated text to a human-written reference and measure the overlap between the two. \n",
    "\n",
    "The metrics range between 0 and 1, with higher scores indicating higher similarity between the baseline and generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create ROUGE metrics\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model results\n",
      "{'rouge1': np.float64(0.17391941391941393), 'rouge2': np.float64(0.03820816864295125), 'rougeL': np.float64(0.13364468864468865), 'rougeLsum': np.float64(0.13364468864468865)}\n",
      "Lora model results\n",
      "{'rouge1': np.float64(0.34193513803269904), 'rouge2': np.float64(0.1024924201890494), 'rougeL': np.float64(0.2728511771470072), 'rougeLsum': np.float64(0.2728511771470072)}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Evaluate the original model's result\n",
    "orig_model_results = rouge.compute(\n",
    "   references=summaries,\n",
    "   predictions=orig_model_summaries,\n",
    "   use_aggregator=True,\n",
    "   use_stemmer=True\n",
    ")\n",
    "\n",
    "lora_model_results = rouge.compute(\n",
    "   references=summaries,\n",
    "   predictions=lora_model_summaries,\n",
    "   use_aggregator=True,\n",
    "   use_stemmer=True\n",
    ")\n",
    "\n",
    "print('Original model results')\n",
    "print(orig_model_results)\n",
    "\n",
    "print('Lora model results')\n",
    "print(lora_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate with Bleu metrics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_workhop_templates",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
