{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 1 - Summarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 Models\n",
    "\n",
    "The <code>flan-t5</code> is a Text-To-Text Transfer Transformer (T5) that is capable of performing zero-shot NLP task such as summary, simple reasoninig, answering questions, etc. \n",
    "\n",
    "Some T5 models from Huggingface\n",
    "- [<code>google/flan-t5-base</code>](https://huggingface.co/google/flan-t5-base)\n",
    "- [<code>google/flan-t5-small</code>](https://huggingface.co/google/flan-t5-small)\n",
    "- [<code>google/flan-t5-xl</code>](https://huggingface.co/google/flan-t5-xl)\n",
    "- [<code>google/flan-t5-xxl</code>](https://huggingface.co/google/flan-t5-xxl) - full model\n",
    "\n",
    "Complete list of [T5 models](https://huggingface.co/models?search=google/flan) on Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/flan-t5-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 768)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-11): 11 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-11): 11 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" \n",
    "Two roads diverged in a yellow wood,\n",
    "And sorry I could not travel both\n",
    "And be one traveler, long I stood\n",
    "And looked down one as far as I could\n",
    "To where it bent in the undergrowth;\n",
    "\n",
    "Then took the other, as just as fair,\n",
    "And having perhaps the better claim,\n",
    "Because it was grassy and wanted wear;\n",
    "Though as for that the passing there\n",
    "Had worn them really about the same,\n",
    "\n",
    "And both that morning equally lay\n",
    "In leaves no step had trodden black.\n",
    "Oh, I kept the first for another day!\n",
    "Yet knowing how way leads on to way,\n",
    "I doubted if I should ever come back.\n",
    "\n",
    "I shall be telling this with a sigh\n",
    "Somewhere ages and ages hence:\n",
    "Two roads diverged in a wood, and Iâ€”\n",
    "I took the one less traveled by,\n",
    "And that has made all the difference.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"When a traveler in north central Massachusetts takes the wrong fork\n",
    "at the junction of the Aylesbury pike just beyond Dean's Corners he\n",
    "comes upon a lonely and curious country. The ground gets higher, and\n",
    "the brier-bordered stone walls press closer and closer against the ruts\n",
    "of the dusty, curving road. The trees of the frequent forest belts\n",
    "seem too large, and the wild weeds, brambles, and grasses attain a\n",
    "luxuriance not often found in settled regions. At the same time the\n",
    "planted fields appear singularly few and barren; while the sparsely\n",
    "scattered houses wear a surprizing uniform aspect of age, squalor, and\n",
    "dilapidation. Without knowing why, one hesitates to ask directions\n",
    "from the gnarled, solitary figures spied now and then on crumbling\n",
    "doorsteps or in the sloping, rock-strewn meadows. Those figures are\n",
    "so silent and furtive that one feels somehow confronted by forbidden\n",
    "things, with which it would be better to have nothing to do. When a\n",
    "rise in the road brings the mountains in view above the deep woods,\n",
    "the feeling of strange uneasiness is increased. The summits are too\n",
    "rounded and symmetrical to give a sense of comfort and naturalness, and\n",
    "sometimes the sky silhouettes with especial clearness the queer circles\n",
    "of tall stone pillars with which most of them are crowned.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a short summary for this text: When a traveler in north central Massachusetts takes the wrong fork\n",
      "at the junction of the Aylesbury pike just beyond Dean's Corners he\n",
      "comes upon a lonely and curious country. The ground gets higher, and\n",
      "the brier-bordered stone walls press closer and closer against the ruts\n",
      "of the dusty, curving road. The trees of the frequent forest belts\n",
      "seem too large, and the wild weeds, brambles, and grasses attain a\n",
      "luxuriance not often found in settled regions. At the same time the\n",
      "planted fields appear singularly few and barren; while the sparsely\n",
      "scattered houses wear a surprizing uniform aspect of age, squalor, and\n",
      "dilapidation. Without knowing why, one hesitates to ask directions\n",
      "from the gnarled, solitary figures spied now and then on crumbling\n",
      "doorsteps or in the sloping, rock-strewn meadows. Those figures are\n",
      "so silent and furtive that one feels somehow confronted by forbidden\n",
      "things, with which it would be better to have nothing to do. When a\n",
      "rise in the road brings the mountains in view above the deep woods,\n",
      "the feeling of strange uneasiness is increased. The summits are too\n",
      "rounded and symmetrical to give a sense of comfort and naturalness, and\n",
      "sometimes the sky silhouettes with especial clearness the queer circles\n",
      "of tall stone pillars with which most of them are crowned.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a prompt\n",
    "prompt = f\"Write a short summary for this text: {text}\"\n",
    "\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 8733,     3,     9,   710,  9251,    21,    48,  1499,    10,   366,\n",
      "             3,     9,  1111,    49,    16,  3457,  2069,  9777,  1217,     8,\n",
      "          1786,    21,   157,    44,     8, 23704,    13,     8,    71,    63,\n",
      "           965,  7165,  2816,  1050,   131,  1909, 12738,    31,     7, 15143,\n",
      "             7,     3,    88,   639,  1286,     3,     9, 23633,    11,  9865,\n",
      "           684,     5,    37,  1591,  2347,  1146,     6,    11,     8,     3,\n",
      "          2160,    49,    18, 24678,    15,    26,  3372,  4205,  2785,  4645,\n",
      "            11,  4645,   581,     8,     3,  6830,     7,    13,     8,  5784,\n",
      "            63,     6,  5495,  3745,  1373,     5,    37,  3124,    13,     8,\n",
      "          8325,  5827,  6782,     7,  1727,   396,   508,     6,    11,     8,\n",
      "          3645,     3,  8578,     7,     6,  3858,    51,  2296,     7,     6,\n",
      "            11,  5956,    15,     7, 14568,     3,     9,     3,  8387,   459,\n",
      "           663,    59,   557,   435,    16, 10166,  6266,     5,   486,     8,\n",
      "           337,    97,     8, 15359,  4120,  2385, 22166,   120,   360,    11,\n",
      "         18595,    29,   117,   298,     8, 14144,     7,    15,   120, 20193,\n",
      "          4790,  2112,     3,     9,   244,  2246,  8128,  7117,  2663,    13,\n",
      "          1246,     6,     3,     7, 11433,   127,     6,    11,  1227,   521,\n",
      "         12417,   257,     5,  6404,  4265,   572,     6,    80, 10888,     7,\n",
      "            12,   987,  7943,    45,     8,     3, 11260,    52,  1361,     6,\n",
      "             3,     7, 28028,  5638,     3,     7,  8082,    26,   230,    11,\n",
      "           258,    30, 22854,    53, 26929,     7,    42,    16,     8,     3,\n",
      "             7,  8745,    53,     6,  2480,    18,     7,   929,   210,    29,\n",
      "           140,     9, 15198,     7,     5,     3,  3405,  5638,    33,    78,\n",
      "         11237,    11,  4223,  3268,    24,    80,  4227,  9296, 11230,    15,\n",
      "            26,    57, 29387,   378,     6,    28,    84,    34,   133,    36,\n",
      "           394,    12,    43,  1327,    12,   103,     5,   366,     3,     9,\n",
      "          3098,    16,     8,  1373,  3200,     8,  8022,    16,   903,   756,\n",
      "             8,  1659,  1679,     7,     6,     8,  1829,    13,  6765,   245,\n",
      "             9,     7,  6096,    19,  1936,     5,    37, 13385,     7,    33,\n",
      "           396,     3, 12279,    11,     3, 23596,   138,    12,   428,     3,\n",
      "             9,  1254,    13,  2115,    11,   793,   655,     6,    11,  1664,\n",
      "             8,  5796, 19561,     7,    28,     3,    15, 17434,   964,   655,\n",
      "             8,   238,    49, 15162,    13,  5065,  3372,     3, 12851,     7,\n",
      "            28,    84,   167,    13,   135,    33,     3, 31657,     5,     3,\n",
      "             1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# TODO: tokenize the text\n",
    "prompt_enc = tokenizer(prompt, return_tensors='pt')\n",
    "print(prompt_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Decode the token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GenerationConfig(\n",
    "   do_sample=True,\n",
    "   temperature=4.0,\n",
    "   min_new_tokens=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,   290,  3223,     6,    28,  6765,     3,  2160,    40,   449,\n",
      "            11,  8227,     7,     5,   366,   728,    46, 13876,   286,  8247,\n",
      "          2347]])\n"
     ]
    }
   ],
   "source": [
    "# TODO: Generate summary with model \n",
    "summary_enc = model.generate(prompt_enc.input_ids, generation_config=config)\n",
    "print(summary_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There exist, with strange brilter and fences. When once an abandoned place suddenly gets\n"
     ]
    }
   ],
   "source": [
    "# TODO: Decode the summary\n",
    "summary = tokenizer.decode(summary_enc[0], skip_special_tokens=True)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When a traveler in north central Massachusetts takes the wrong fork\n",
      "at the junction of the Aylesbury pike just beyond Dean's Corners he\n",
      "comes upon a lonely and curious country. The ground gets higher, and\n",
      "the brier-bordered stone walls press closer and closer against the ruts\n",
      "of the dusty, curving road. The trees of the frequent forest belts\n",
      "seem too large, and the wild weeds, brambles, and grasses attain a\n",
      "luxuriance not often found in settled regions. At the same time the\n",
      "planted fields appear singularly few and barren; while the sparsely\n",
      "scattered houses wear a surprizing uniform aspect of age, squalor, and\n",
      "dilapidation. Without knowing why, one hesitates to ask directions\n",
      "from the gnarled, solitary figures spied now and then on crumbling\n",
      "doorsteps or in the sloping, rock-strewn meadows. Those figures are\n",
      "so silent and furtive that one feels somehow confronted by forbidden\n",
      "things, with which it would be better to have nothing to do. When a\n",
      "rise in the road brings the mountains in view above the deep woods,\n",
      "the feeling of strange uneasiness is increased. The summits are too\n",
      "rounded and symmetrical to give a sense of comfort and naturalness, and\n",
      "sometimes the sky silhouettes with especial clearness the queer circles\n",
      "of tall stone pillars with which most of them are crowned.\n",
      "\n",
      "What is the travelle's occupation? (If the question is unanswerable, say 'unanswerable')\n"
     ]
    }
   ],
   "source": [
    "question = \"Where is the traveller travelling in?\"\n",
    "question = \"What is the travelle's occupation?\"\n",
    "prompt = f\"{text}\\n{question} (If the question is unanswerable, say 'unanswerable')\" \n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_enc = tokenizer(prompt, return_tensors='pt').input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_enc = model.generate(prompt_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unanswerable\n"
     ]
    }
   ],
   "source": [
    "answer = tokenizer.decode(answer_enc[0], skip_special_tokens=True)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_workhop_templates",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
